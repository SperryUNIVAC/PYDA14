{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as req\n",
    "import pandas as pd \n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = ['AMD', 'Intel', 'IBM', 'Oracle', 'Google', 'Apple', 'Microsoft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_change(string):\n",
    "    date = r'(\\d{4}-\\d{2}-\\d{2}) \\d{2}:\\d{2}:\\d{2}.\\d{6}сегодня в (\\d{2}:\\d{2})'\n",
    "    replace = r'\\1 \\2'\n",
    "    return re.sub(date, replace, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_test(string):\n",
    "    time = r'(.+) в \\d{2}:\\d{2}'\n",
    "    replace = r'\\1'\n",
    "    return re.sub(time, replace, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(string):\n",
    "    tags = r'\\\\r|\\\\n'\n",
    "    replace = r' '\n",
    "    return re.sub(tags, replace, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### распарсим одну страницу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = str(datetime.datetime.now())\n",
    "parsing_dict_list = []\n",
    "# список из превью по странице\n",
    "post_preview = soup.find_all(class_='post post_preview')\n",
    "\n",
    "for post in post_preview:\n",
    "    # вытащить превью\n",
    "    link_preview = post.find_all(class_='post__title_link')\n",
    "    if link_preview:\n",
    "        # выташить линки и время\n",
    "        url = link_preview[0].get('href')\n",
    "        time_preview = post.find_all(class_='post__time')\n",
    "    # еще один вариант нужных нам ссылок\n",
    "    else:\n",
    "        link_preview = post.find_all(class_='preview-data__title-link')\n",
    "        time_preview = post.find_all(class_='preview-data__time-published')\n",
    "    url = link_preview[0].get('href')\n",
    "    time = time_preview[0].get_text()\n",
    "    header = link_preview[0].get_text()\n",
    "    text = post.get_text()\n",
    "    date_news = date_change(time_now+time)\n",
    "    for key in KEYWORDS:\n",
    "        if text.find(key) > 0:\n",
    "            record = {'Tag':key, 'Date':date_news, 'URL':url, 'Header':header}\n",
    "            parsing_dict_list.append(record)\n",
    "\n",
    "pd.DataFrame(parsing_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### а теперь самое интересное - вытащим со всех страниц новостей только сегодняшние новости с требуемыми компаниями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = str(datetime.datetime.now())\n",
    "parsing_dict_list = []\n",
    "i = 1\n",
    "timestamp = 'сегодня'\n",
    "\n",
    "while timestamp == 'сегодня':\n",
    "    \n",
    "    # самое забавное, что я был немного не в форме во время лекции, здоровье подвело, и момент с передачей параметров \n",
    "    # я как-то вообще пролюбил и только когда уже пересматривал потом запись, врубился в технику посылания запросов с ними\n",
    "    # так что переключать страницы можно и покрасивее возможно, но этот код я написал еще до этого, просто придумав свое\n",
    "    # немного костыльное решение с формированием URL по ходу дела и оно сработало!\n",
    "    # ну раз работает, я решил не переделывать этот кусок в мемориальных целях\n",
    "    \n",
    "    url = 'https://habr.com/ru/all/page' + str(i)\n",
    "    req_page = req.get(url)\n",
    "    soup = BeautifulSoup(req_page.text, 'html.parser')\n",
    "    post_preview = soup.find_all(class_='post post_preview')\n",
    "    for post in post_preview:\n",
    "        link_preview = post.find_all(class_='post__title_link')\n",
    "        if link_preview: \n",
    "        # нужный нам элемент выдирается разными способами в зависимости от того статья это или часть корпоративного блога\n",
    "            url = link_preview[0].get('href')\n",
    "        # тег div я тоже явно не прописывал, но оно все лежит и так там, где описано\n",
    "            time_preview = post.find_all(class_='post__time')\n",
    "        else:\n",
    "            link_preview = post.find_all(class_='preview-data__title-link')\n",
    "            time_preview = post.find_all(class_='preview-data__time-published')\n",
    "        url = link_preview[0].get('href')\n",
    "        time = time_preview[0].get_text()\n",
    "        header = link_preview[0].get_text()\n",
    "        text = post.get_text()\n",
    "        date_news = date_change(time_now+time)\n",
    "        timestamp = time_test(time)\n",
    "        if timestamp == 'сегодня':\n",
    "            for key in KEYWORDS:\n",
    "                if text.find(key) > 0:\n",
    "                    record = {'Tag':key, 'Date':date_news, 'URL':url, 'Header':header}\n",
    "                    parsing_dict_list.append(record)\n",
    "        else:\n",
    "            break\n",
    "    i += 1\n",
    "pd.DataFrame(parsing_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### теперь попробуем распарсить текст статьи по каждой ссылке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = str(datetime.datetime.now())\n",
    "parsing_dict_list = []\n",
    "i = 1\n",
    "timestamp = 'сегодня'\n",
    "\n",
    "while timestamp == 'сегодня':\n",
    "    url = 'https://habr.com/ru/all/page' + str(i)\n",
    "    req_page = req.get(url)\n",
    "    soup = BeautifulSoup(req_page.text, 'html.parser')\n",
    "    post_preview = soup.find_all(class_='post post_preview')\n",
    "    for post in post_preview:\n",
    "        link_preview = post.find_all(class_='post__title_link')\n",
    "        if link_preview:\n",
    "            url = link_preview[0].get('href')\n",
    "            time_preview = post.find_all(class_='post__time')\n",
    "        else:\n",
    "            link_preview = post.find_all(class_='preview-data__title-link')\n",
    "            time_preview = post.find_all(class_='preview-data__time-published')\n",
    "        url = link_preview[0].get('href')\n",
    "        time = time_preview[0].get_text()\n",
    "        header = link_preview[0].get_text()\n",
    "        date_news = date_change(time_now+time)\n",
    "        timestamp = time_test(time)       \n",
    "        # заход на страницу\n",
    "        req_page_inner = req.get(url)\n",
    "        soup_inner = BeautifulSoup(req_page_inner.text, 'html.parser')\n",
    "        full_text = soup_inner.find_all(id=\"post-content-body\")\n",
    "        html = full_text[0].get_text()\n",
    "        clean_text = clean_html(html)\n",
    "        if timestamp == 'сегодня':\n",
    "            for key in KEYWORDS:\n",
    "                if clean_text.find(key) > 0:\n",
    "                    record = {'Tag':key, 'Date':date_news, 'URL':url, 'Full text':clean_text}\n",
    "                    parsing_dict_list.append(record)\n",
    "        else:\n",
    "            break\n",
    "    i += 1\n",
    "pd.DataFrame(parsing_dict_list)\n",
    "# какого-то фига у меня не сработал регексп, полностью очищающий от всяких \\n\\n хотя \n",
    "# в regexp101 он нормально чистит, я проверял"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь поколупаем скрытые API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = ['xxx@x.ru', 'yyy@y.com', 'zzz@x.ru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "headers_dict = {\n",
    "'Connection': 'keep-alive',\n",
    "'Content-Type': 'application/json;charset=UTF-8',\n",
    "'Host': 'identityprotection.avast.com',\n",
    "'Origin': 'https://www.avast.com',\n",
    "'Referer': 'https://www.avast.com/',\n",
    "'Sec-Fetch-Dest': 'empty',\n",
    "'Sec-Fetch-Mode': 'cors',\n",
    "'Sec-Fetch-Site': 'same-site',\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36',\n",
    "'Vaar-Header-App-Product': 'hackcheck-web-avast',\n",
    "'Vaar-Version': '0'\n",
    "}\n",
    "\n",
    "compromised_dict = {}\n",
    "for email in EMAIL:\n",
    "    payload = {\"emailAddresses\":[email]}\n",
    "    response = req.post(url, json=payload, headers=headers_dict)\n",
    "    data = response.json()\n",
    "    if data['data'].keys():\n",
    "        compromised_email = list(data['data'].keys())\n",
    "        compromised_dict.setdefault(email, compromised_email)\n",
    "    else:\n",
    "        compromised_dict.setdefault(email, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
