{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as req\n",
    "import pandas as pd \n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = ['AMD', 'Intel', 'IBM', 'Oracle', 'Google', 'Apple', 'Microsoft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_change(string):\n",
    "    date = r'(\\d{4}-\\d{2}-\\d{2}) \\d{2}:\\d{2}:\\d{2}.\\d{6}сегодня в (\\d{2}:\\d{2})'\n",
    "    replace = r'\\1 \\2'\n",
    "    return re.sub(date, replace, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_test(string):\n",
    "    time = r'(.+) в \\d{2}:\\d{2}'\n",
    "    replace = r'\\1'\n",
    "    return re.sub(time, replace, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(string):\n",
    "    tags = r'\\\\r|\\\\n'\n",
    "    replace = r' '\n",
    "    return re.sub(tags, replace, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### распарсим одну страницу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9d44d18028da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparsing_dict_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# список из превью по странице\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpost_preview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post post_preview'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpost_preview\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "time_now = str(datetime.datetime.now())\n",
    "parsing_dict_list = []\n",
    "# список из превью по странице\n",
    "post_preview = soup.find_all(class_='post post_preview')\n",
    "\n",
    "for post in post_preview:\n",
    "    # вытащить превью\n",
    "    link_preview = post.find_all(class_='post__title_link')\n",
    "    if link_preview:\n",
    "        # выташить линки и время\n",
    "        url = link_preview[0].get('href')\n",
    "        time_preview = post.find_all(class_='post__time')\n",
    "    # еще один вариант нужных нам ссылок\n",
    "    else:\n",
    "        link_preview = post.find_all(class_='preview-data__title-link')\n",
    "        time_preview = post.find_all(class_='preview-data__time-published')\n",
    "    url = link_preview[0].get('href')\n",
    "    time = time_preview[0].get_text()\n",
    "    header = link_preview[0].get_text()\n",
    "    text = post.get_text()\n",
    "    date_news = date_change(time_now+time)\n",
    "    for key in KEYWORDS:\n",
    "        if text.find(key) > 0:\n",
    "            record = {'Tag':key, 'Date':date_news, 'URL':url, 'Header':header}\n",
    "            parsing_dict_list.append(record)\n",
    "\n",
    "pd.DataFrame(parsing_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### а теперь самое интересное - вытащим со всех страниц новостей только сегодняшние новости с требуемыми компаниями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Header</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>2020-11-20 11:55</td>\n",
       "      <td>https://habr.com/ru/company/vdsina/blog/528990/</td>\n",
       "      <td>Окей, Гугл, опубликуй свои секретные ключи DKIM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google</td>\n",
       "      <td>2020-11-20 03:55</td>\n",
       "      <td>https://habr.com/ru/company/selectel/blog/529010/</td>\n",
       "      <td>Ubuntu Web Remix — альтернатива Chrome OS c бр...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tag              Date  \\\n",
       "0  Google  2020-11-20 11:55   \n",
       "1  Google  2020-11-20 03:55   \n",
       "\n",
       "                                                 URL  \\\n",
       "0    https://habr.com/ru/company/vdsina/blog/528990/   \n",
       "1  https://habr.com/ru/company/selectel/blog/529010/   \n",
       "\n",
       "                                              Header  \n",
       "0    Окей, Гугл, опубликуй свои секретные ключи DKIM  \n",
       "1  Ubuntu Web Remix — альтернатива Chrome OS c бр...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_now = str(datetime.datetime.now())\n",
    "parsing_dict_list = []\n",
    "i = 1\n",
    "timestamp = 'сегодня'\n",
    "\n",
    "while timestamp == 'сегодня':\n",
    "    \n",
    "    # самое забавное, что я был немного не в форме во время лекции, здоровье подвело, и момент с передачей параметров \n",
    "    # я как-то вообще пролюбил и только когда уже пересматривал потом запись, врубился в технику посылания запросов с ними\n",
    "    # так что переключать страницы можно и покрасивее возможно, но этот код я написал еще до этого, просто придумав свое\n",
    "    # немного костыльное решение с формированием URL по ходу дела и оно сработало!\n",
    "    # ну раз работает, я решил не переделывать этот кусок в мемориальных целях\n",
    "    \n",
    "    url = 'https://habr.com/ru/all/page' + str(i)\n",
    "    req_page = req.get(url)\n",
    "    soup = BeautifulSoup(req_page.text, 'html.parser')\n",
    "    post_preview = soup.find_all(class_='post post_preview')\n",
    "    for post in post_preview:\n",
    "        link_preview = post.find_all(class_='post__title_link')\n",
    "        if link_preview: \n",
    "        # нужный нам элемент выдирается разными способами в зависимости от того статья это или часть корпоративного блога\n",
    "            url = link_preview[0].get('href')\n",
    "        # тег div я тоже явно не прописывал, но оно все лежит и так там, где описано\n",
    "            time_preview = post.find_all(class_='post__time')\n",
    "        else:\n",
    "            link_preview = post.find_all(class_='preview-data__title-link')\n",
    "            time_preview = post.find_all(class_='preview-data__time-published')\n",
    "        url = link_preview[0].get('href')\n",
    "        time = time_preview[0].get_text()\n",
    "        header = link_preview[0].get_text()\n",
    "        text = post.get_text()\n",
    "        date_news = date_change(time_now+time)\n",
    "        timestamp = time_test(time)\n",
    "        if timestamp == 'сегодня':\n",
    "            for key in KEYWORDS:\n",
    "                if text.find(key) > 0:\n",
    "                    record = {'Tag':key, 'Date':date_news, 'URL':url, 'Header':header}\n",
    "                    parsing_dict_list.append(record)\n",
    "        else:\n",
    "            break\n",
    "    i += 1\n",
    "pd.DataFrame(parsing_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### теперь попробуем распарсить текст статьи по каждой ссылке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Full text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>2020-11-20 12:27</td>\n",
       "      <td>https://habr.com/ru/post/528914/</td>\n",
       "      <td>Я проработал в сфере IT около 25 лет, из котор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google</td>\n",
       "      <td>2020-11-20 11:55</td>\n",
       "      <td>https://habr.com/ru/company/vdsina/blog/528990/</td>\n",
       "      <td>\\n\\r\\nИнтернет даже в лучшие свои годы был опа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>2020-11-20 11:50</td>\n",
       "      <td>https://habr.com/ru/post/528950/</td>\n",
       "      <td>Кто сегодня не слышал про вебинары – реалии се...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google</td>\n",
       "      <td>2020-11-20 11:26</td>\n",
       "      <td>https://habr.com/ru/post/529004/</td>\n",
       "      <td>\\n\\n&lt;&lt; До этого: Восхождение интернета, ч.2: п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple</td>\n",
       "      <td>2020-11-20 11:26</td>\n",
       "      <td>https://habr.com/ru/post/529004/</td>\n",
       "      <td>\\n\\n&lt;&lt; До этого: Восхождение интернета, ч.2: п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>2020-11-20 11:26</td>\n",
       "      <td>https://habr.com/ru/post/529004/</td>\n",
       "      <td>\\n\\n&lt;&lt; До этого: Восхождение интернета, ч.2: п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Google</td>\n",
       "      <td>2020-11-20 11:14</td>\n",
       "      <td>https://habr.com/ru/company/jugru/blog/527508/</td>\n",
       "      <td>Soft skills крайне важны для DevOps-специалист...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Google</td>\n",
       "      <td>2020-11-20 10:51</td>\n",
       "      <td>https://habr.com/ru/company/mailru/blog/528960/</td>\n",
       "      <td>\\n\\r\\nВсем привет! В нашем совместном с МФТИ у...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Intel</td>\n",
       "      <td>2020-11-20 10:50</td>\n",
       "      <td>https://habr.com/ru/company/ua-hosting/blog/52...</td>\n",
       "      <td>\\n\\r\\nЧто общего между молоком, сливочным масл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Intel</td>\n",
       "      <td>2020-11-20 10:13</td>\n",
       "      <td>https://habr.com/ru/post/529020/</td>\n",
       "      <td>\\n\\r\\nВ данной публикации будет описан немного...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Google</td>\n",
       "      <td>2020-11-20 03:55</td>\n",
       "      <td>https://habr.com/ru/company/selectel/blog/529010/</td>\n",
       "      <td>\\r\\nХромбуки — популярные устройства, продажи ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Tag              Date  \\\n",
       "0   Microsoft  2020-11-20 12:27   \n",
       "1      Google  2020-11-20 11:55   \n",
       "2       Apple  2020-11-20 11:50   \n",
       "3      Google  2020-11-20 11:26   \n",
       "4       Apple  2020-11-20 11:26   \n",
       "5   Microsoft  2020-11-20 11:26   \n",
       "6      Google  2020-11-20 11:14   \n",
       "7      Google  2020-11-20 10:51   \n",
       "8       Intel  2020-11-20 10:50   \n",
       "9       Intel  2020-11-20 10:13   \n",
       "10     Google  2020-11-20 03:55   \n",
       "\n",
       "                                                  URL  \\\n",
       "0                    https://habr.com/ru/post/528914/   \n",
       "1     https://habr.com/ru/company/vdsina/blog/528990/   \n",
       "2                    https://habr.com/ru/post/528950/   \n",
       "3                    https://habr.com/ru/post/529004/   \n",
       "4                    https://habr.com/ru/post/529004/   \n",
       "5                    https://habr.com/ru/post/529004/   \n",
       "6      https://habr.com/ru/company/jugru/blog/527508/   \n",
       "7     https://habr.com/ru/company/mailru/blog/528960/   \n",
       "8   https://habr.com/ru/company/ua-hosting/blog/52...   \n",
       "9                    https://habr.com/ru/post/529020/   \n",
       "10  https://habr.com/ru/company/selectel/blog/529010/   \n",
       "\n",
       "                                            Full text  \n",
       "0   Я проработал в сфере IT около 25 лет, из котор...  \n",
       "1   \\n\\r\\nИнтернет даже в лучшие свои годы был опа...  \n",
       "2   Кто сегодня не слышал про вебинары – реалии се...  \n",
       "3   \\n\\n<< До этого: Восхождение интернета, ч.2: п...  \n",
       "4   \\n\\n<< До этого: Восхождение интернета, ч.2: п...  \n",
       "5   \\n\\n<< До этого: Восхождение интернета, ч.2: п...  \n",
       "6   Soft skills крайне важны для DevOps-специалист...  \n",
       "7   \\n\\r\\nВсем привет! В нашем совместном с МФТИ у...  \n",
       "8   \\n\\r\\nЧто общего между молоком, сливочным масл...  \n",
       "9   \\n\\r\\nВ данной публикации будет описан немного...  \n",
       "10  \\r\\nХромбуки — популярные устройства, продажи ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_now = str(datetime.datetime.now())\n",
    "parsing_dict_list = []\n",
    "i = 1\n",
    "timestamp = 'сегодня'\n",
    "\n",
    "while timestamp == 'сегодня':\n",
    "    url = 'https://habr.com/ru/all/page' + str(i)\n",
    "    req_page = req.get(url)\n",
    "    soup = BeautifulSoup(req_page.text, 'html.parser')\n",
    "    post_preview = soup.find_all(class_='post post_preview')\n",
    "    for post in post_preview:\n",
    "        link_preview = post.find_all(class_='post__title_link')\n",
    "        if link_preview:\n",
    "            url = link_preview[0].get('href')\n",
    "            time_preview = post.find_all(class_='post__time')\n",
    "        else:\n",
    "            link_preview = post.find_all(class_='preview-data__title-link')\n",
    "            time_preview = post.find_all(class_='preview-data__time-published')\n",
    "        url = link_preview[0].get('href')\n",
    "        time = time_preview[0].get_text()\n",
    "        header = link_preview[0].get_text()\n",
    "        date_news = date_change(time_now+time)\n",
    "        timestamp = time_test(time)       \n",
    "        # заход на страницу\n",
    "        req_page_inner = req.get(url)\n",
    "        soup_inner = BeautifulSoup(req_page_inner.text, 'html.parser')\n",
    "        full_text = soup_inner.find_all(id=\"post-content-body\")\n",
    "        html = full_text[0].get_text()\n",
    "        clean_text = clean_html(html)\n",
    "        if timestamp == 'сегодня':\n",
    "            for key in KEYWORDS:\n",
    "                if clean_text.find(key) > 0:\n",
    "                    record = {'Tag':key, 'Date':date_news, 'URL':url, 'Full text':clean_text}\n",
    "                    parsing_dict_list.append(record)\n",
    "        else:\n",
    "            break\n",
    "    i += 1\n",
    "pd.DataFrame(parsing_dict_list)\n",
    "# какого-то фига у меня не сработал регексп, полностью очищающий от всяких \\n\\n хотя \n",
    "# в regexp101 он нормально чистит, я проверял"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь поколупаем скрытые API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = ['xxx@x.ru', 'yyy@y.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.avast.com/hackcheck/'\n",
    "data = {\"emailAddresses\":[EMAIL[0]]}\n",
    "\n",
    "headers = {\n",
    "'Connection': 'keep-alive',\n",
    "'Content-Type': 'application/json;charset=UTF-8',\n",
    "'Host': 'identityprotection.avast.com',\n",
    "'Origin': 'https://www.avast.com',\n",
    "'Referer': 'https://www.avast.com/',\n",
    "'Sec-Fetch-Dest': 'empty',\n",
    "'Sec-Fetch-Mode': 'cors',\n",
    "'Sec-Fetch-Site': 'same-site',\n",
    "'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36',\n",
    "'Vaar-Header-App-Product': 'hackcheck-web-avast',\n",
    "'Vaar-Version': '0'\n",
    "}\n",
    "\n",
    "response = req.post(url, data, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [400]>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем я извращался над запросом ВСЕМИ способами просто - пихал нагрузку в json и не пихал, копировал все что можно из заголовка, ну как бы больше вариантов я, увы, не знаю, ибо в сайтостроении я шарю исключительно в самом самой самой общей теории и чего еще ему от меня надо я понять не в состоянии. Куков он никаких не ждет и не отправляет, я смотрел в инспекторе,но даже если бы и ждал - я не нашел ни одного адекватного туториала, как их из тормозиллы выколупать и прилепить к запросу. Все ответы на вопросы во всяких чатах обычно подразумевают уже нехилое знание спрашивающим веба. Так что, увы, вебмастер из меня нулевой абсолютно и я даже реально вообще не в состоянии осознать - чего он в приниципе еще может хотеть от меня??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
